This notenook contains from scratch implementation of Activation functions
The following are covered:
1. Sigmoid
2. Tanh
3. ReLU
4. Leaky ReLU
5. ELU

Maxout is shown, but not implemented because all the rest can be shown with just one tensor. However, thats not the case with Maxout, and it can be implemented easily with weight and biases, formula is given.
